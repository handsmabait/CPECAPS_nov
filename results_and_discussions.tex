% results_and_discussions.tex
% Included by document.tex which contains the \chapter{Results and Discussions}

% Included by document.tex which contains the \chapter{Results and Discussions}
\label{ch:result_discuss}
\label{sec:results_quant}

\subsection{Leg Landmark Detection Results}
\label{sec:results_quant}

The implementation of the leg tracking system successfully demonstrates the capability to detect and track key anatomical landmarks on the lower extremities. Figure \ref{fig:leg_landmark} illustrates the detected landmarks overlaid on the leg region, showing the system's ability to identify critical points such as the hip, knee, and ankle joints.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{legtracker.png}
    \caption{Leg Landmark Estimation showing detected keypoints on lower extremities}
    \label{fig:leg_landmark}
\end{figure}

The landmark detection forms the foundation for subsequent gait analysis, as these keypoints enable the calculation of joint angles, stride length, and other biomechanical parameters essential for assessing walking patterns.

\section{Training Dataset}

The training dataset comprises video frames captured from various walking scenarios to ensure robust model performance across different conditions. Figures \ref{fig:train_sample_1} through \ref{fig:train_sample_3} present representative samples from the training dataset, demonstrating the diversity of poses, lighting conditions, and perspectives included in the model training process.

% fraame extraction

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{frame_000040.jpg}
    \caption{Training data sample illustrating }
    \label{fig:train_sample_3}
\end{figure}


%live prediction
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{live_prediction.png}
    \caption{Live prediction sample during application runtime}
    \label{fig:train_sample_1}
\end{figure}
%model evaluation
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{confusion_matrix_moves.png}
    \caption{Confusion matrix for movement classification}
    \label{fig:train_sample_1}
\end{figure}
%model evaluation  
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{confusion_matrix_quality.png}
    \caption{Confusion matrix for movement classification}
    \label{fig:train_sample_1}
\end{figure}

\subsection{Model Evaluation and Discussion}
\label{sec:results_robustness}
The developed pose-based movement classification model was evaluated using the collected video data and corresponding ground-truth annotations. The results demonstrate the system’s ability to recognize leg movement patterns and assess the quality of performance with reasonable accuracy.

Figure~\ref{fig:train_sample_1} illustrates a live prediction sample captured during runtime, showing the model’s ability to process incoming video frames in real time. The overlayed labels indicate the detected dance movement and its corresponding quality classification (e.g., \textit{excellent}, \textit{good}). This confirms that the inference pipeline can operate interactively, making it suitable for applications such as performance feedback or dance training systems.

To quantitatively assess the performance, confusion matrices were generated for both movement classification and quality evaluation, as shown in Figures~\ref{fig:train_sample_1} and~\ref{fig:train_sample_1}. The confusion matrix for movement classification shows that the model achieves strong discriminative performance across most of the defined movement categories, with most predictions aligning closely with their ground-truth counterparts. Misclassifications were observed primarily between movements with similar leg trajectories or temporal overlap, such as \textit{touch step} and \textit{hop step} variations. This overlap suggests that temporal smoothing or additional motion cues (e.g., velocity vectors) could further enhance differentiation.

Meanwhile, the confusion matrix for quality classification demonstrates that the model is capable of distinguishing general performance levels but occasionally confuses borderline cases between \textit{good} and \textit{excellent}. This behavior is likely due to the limited size and subjective labeling of the dataset, where visual differences between these categories may be subtle. Future iterations could benefit from a larger dataset with finer-grained quality annotations and more consistent labeling criteria.

Overall, the evaluation confirms that the proposed system is effective in identifying leg movements and providing qualitative feedback. The results highlight the potential of pose estimation and lightweight machine learning models in automating dance movement assessment, while also identifying key areas for improvement such as dataset expansion, model regularization, and temporal fusion strategies.
\begin{table}[htbp]
    \centering
    \caption{Overall model evaluation metrics for movement and quality classification}
    \label{tab:model_metrics}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Metric} & \textbf{Description} & \textbf{Accuracy (\%)} \\
        \midrule
        Movement Classification & Correctly identified dance movement type & 65.74 \\
        Quality Classification & Correctly identified performance quality label & 84.63 \\
        \midrule
        Total Matched Frames & Frames aligned with ground truth annotations & 246 \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Latency and Feedback Timing}
\label{sec:results_latency}
% Show in this chapter proofs why your proposed solution works.  However, presenting results ("It worked") without an appropriate explanation does not show thorough understanding.  Aside from the data and results that you have obtained, and their explanation, the discussion includes why components of your proposed solution worked or did not work in accordance to what you described in the evaluation process, and how the proposed solution performed and fared. Interpret the results and the reasons why they were obtained.  If your results are incorrect, apparent discrepancies from theory should be pointed out and explained. In essence, what do the results mean?  Citing existing publication can help you compare your results and your explanations.

% The next items below are not related to the description of this results and discussions chapter, but serve as an opener for the \LaTeX\ portion of this template.

% Here is an example of a citation for ISO~80000-2 standard~\cite{ISO800002}. Another one is~\cite{Einstein} and~\cite{croft-78}. 

% In using this template, the user is expected to have a working knowledge of \LaTeX. A good introduction is in~\cite{Oetiker2014}.  Its latest version can be accessed at \url{http://www.ctan.org/tex-archive/info/lshort}. See the Appendix of \verb|document_guide.pdf| for examples.

% In aggregate form, Table~\ref{tab:outcomes_per_objective} shows the outcomes and completions in applying the methodology of the \documentType\ per objective.

% ---- corrected longtable (multi-page) ----
\renewcommand{\arraystretch}{1.0} % tighten vertical spacing for table
\setlength{\tabcolsep}{6pt}      % tighten horizontal padding

\begin{SingleSpace} % keep table single-spaced
\begin{longtable}{|p{0.24\textwidth}|p{0.40\textwidth}|p{0.14\textwidth}|}
\caption{Summary of results for achieving the objectives}
\label{tab:outcomes_per_objective} \\

\hline
\textbf{Objectives} & \textbf{Results} & \textbf{Locations} \\ \hline
\endfirsthead

\multicolumn{3}{c}{{\bfseries Table \thetable\ (continued)}} \\ \hline
\textbf{Objectives} & \textbf{Results} & \textbf{Locations} \\ \hline
\endhead

\hline \multicolumn{3}{r}{{Continued on next page}} \\ \hline
\endfoot

\hline
\endlastfoot

\hline
\Paste{GO} &
\begin{enumerate}
  \item Application prototype implemented (desktop).
  \item Integration: MediaPipe + OpenCV + GUI framework completed.
  \item Documentation: architecture, usage, installer prepared.
\end{enumerate}
& Sec.~\ref{sec:implement} on p.~\pageref{sec:implement} \\ \hline

\Paste{SO1} &
\begin{enumerate}
  \item Real-time pipeline achieving target fps and detection accuracy (reported in Sec.~\ref{sec:results_quant}).
  \item Preprocessing and optimization applied.
  \item Accuracy/evaluation results in Table~\ref{tab:accuracy_results}.
\end{enumerate}
& Sec.~\ref{sec:implement} on p.~\pageref{sec:implement} \\ \hline

\Paste{SO2} &
\begin{enumerate}
  \item Dataset collection under diverse conditions completed.
  \item Augmentation and retraining produced measured robustness gains.
  \item Validation metrics summarized in Sec.~\ref{sec:results_robustness}.
\end{enumerate}
& Sec.~\ref{sec:implement} on p.~\pageref{sec:implement} \\ \hline

\Paste{SO3} &
\begin{enumerate}
  \item Scoring and feedback engine implemented; per-segment reports generated.
  \item Latency measurements and UI timing logged (see Sec.~\ref{sec:results_latency}).
\end{enumerate}
& Sec.~\ref{sec:implement} on p.~\pageref{sec:implement} \\ \hline

\Paste{SO4} &
\begin{enumerate}
  \item User study (n~\(\geq\)~10) conducted; user satisfaction and metrics collected.
  \item Evaluation report compiled with recommendations.
\end{enumerate}
& Sec.~\ref{sec:implement} on p.~\pageref{sec:implement} \\ \hline

\end{longtable}
\end{SingleSpace}
% ---- end corrected longtable ----


\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\
\hline
cross\_left & 0.60 & 0.62 & 0.61 & 13 \\
cross\_right & 0.63 & 0.64 & 0.63 & 14 \\
hop\_step\_left & 0.58 & 0.60 & 0.59 & 23 \\
hop\_step\_right & 0.61 & 0.59 & 0.60 & 27 \\
jack\_step & 0.65 & 0.62 & 0.64 & 44 \\
touch\_step\_left & 0.68 & 0.70 & 0.69 & 42 \\
touch\_step\_right & 0.70 & 0.72 & 0.71 & 36 \\
waltz\_left & 0.55 & 0.54 & 0.55 & 11 \\
waltz\_left\_neutral & 0.57 & 0.56 & 0.56 & 12 \\
waltz\_right & 0.56 & 0.55 & 0.56 & 12 \\
waltz\_right\_neutral & 0.58 & 0.57 & 0.57 & 12 \\
\hline
\textbf{Accuracy} & \multicolumn{3}{c}{0.65} & 246 \\
\textbf{Macro avg} & 0.62 & 0.62 & 0.62 & 246 \\
\textbf{Weighted avg} & 0.65 & 0.65 & 0.65 & 246 \\
\hline
\end{tabular}
\caption{Classification report for dance move prediction.}
\label{tab:classification_report}
\end{table}
The classification report in Table~\ref{tab:classification_report} shows an overall accuracy of 0.65 for the dance move prediction task. The weighted averages of precision, recall, and F1-score align closely with the overall accuracy, indicating a reasonably balanced performance across all classes.

Individual class performance reveals that "touch\_step\_right" achieved the highest F1-score of 0.71, reflecting the model's strong capability in recognizing this move. In contrast, "waltz\_left" shows the lowest F1-score of 0.55, suggesting difficulties in distinguishing this class from visually similar moves. The macro averages (precision, recall, and F1-score ≈ 0.62) are slightly lower than the weighted averages, indicating that performance is better on classes with larger sample sizes, such as "jack\_step" and "touch\_step\_left," while underperforming on less frequent ones.

Overall, the statistical


% \graytx{\Blindtext}

\section{Summary}
This chapter presented the implementation results and evaluation of the proposed leg movement recognition and quality assessment system. The leg landmark detection successfully identified key anatomical points on the lower extremities, forming the foundation for motion tracking and gait analysis. A diverse training dataset ensured model robustness across various poses and lighting conditions, supporting accurate movement classification during real-time operation.

Model evaluation demonstrated that the system achieved an overall accuracy of 65\% for movement classification and 84.63\% for quality assessment. Confusion matrices confirmed the model’s ability to distinguish between most dance movements, with misclassifications occurring primarily among visually similar patterns. Statistical analysis showed balanced performance across classes, though accuracy was higher for frequently represented movements such as \textit{touch_step} and \textit{jack_step}.

These results validate the effectiveness of the proposed pose-based approach for automated dance analysis. However, they also highlight the need for further refinement through dataset expansion, temporal modeling, and class balancing to improve recognition of underrepresented movements. Overall, the findings confirm that lightweight pose-driven models can reliably support interactive movement feedback and performance evaluation systems.
